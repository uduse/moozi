{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "import functools\n",
    "from pprint import pprint\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, value_and_grad, jit, vmap\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental import stax\n",
    "import optax\n",
    "import haiku as hk\n",
    "from jax.tree_util import tree_flatten\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pyspiel\n",
    "import numpy as np\n",
    "import trueskill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run mozi/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game('tic_tac_toe')\n",
    "key = jax.random.PRNGKey(0)\n",
    "dim_image = game.observation_tensor_size()\n",
    "dim_repr = 4\n",
    "dim_actions = game.num_distinct_actions()\n",
    "all_actions = list(range(dim_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, true):\n",
    "    return jnp.mean(jnp.square(pred - true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(hk.Module):\n",
    "    def repr_net(self, image):\n",
    "        net = hk.nets.MLP(output_sizes=[16, 16, dim_repr], name='repr')\n",
    "        return net(image)\n",
    "\n",
    "    def pred_net(self, hidden_state):\n",
    "        v_net = hk.nets.MLP(output_sizes=[16, 16, 1], name='pred_v')\n",
    "        p_net = hk.nets.MLP(output_sizes=[16, 16, dim_actions], name='pred_p')\n",
    "        return v_net(hidden_state), p_net(hidden_state)\n",
    "\n",
    "    def dyna_net(self, hidden_state, action):\n",
    "        state_action_repr = jnp.concatenate((hidden_state, action), axis=-1)\n",
    "        transition_net = hk.nets.MLP(\n",
    "            output_sizes=[16, 16, dim_repr], name='dyna_trans')\n",
    "        reward_net = hk.nets.MLP(\n",
    "            output_sizes=[16, 16, dim_repr], name='dyna_reward')\n",
    "        return transition_net(state_action_repr), reward_net(state_action_repr)\n",
    "\n",
    "    def initial_inference(self, image):\n",
    "        hidden_state = self.repr_net(image)\n",
    "        reward = 0\n",
    "        value, policy_logits = self.pred_net(hidden_state)\n",
    "        return NetworkOutput(\n",
    "            value=value,\n",
    "            reward=reward,\n",
    "            policy_logits=policy_logits,\n",
    "            hidden_state=hidden_state\n",
    "        )\n",
    "\n",
    "    def recurrent_inference(self, hidden_state, action):\n",
    "        hidden_state, reward = self.dyna_net(hidden_state, action)\n",
    "        value, policy_logits = self.pred_net(hidden_state)\n",
    "        return NetworkOutput(\n",
    "            value=value,\n",
    "            reward=reward,\n",
    "            policy_logits=policy_logits,\n",
    "            hidden_state=hidden_state\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_inference = hk.without_apply_rng(hk.transform(lambda x: Model().initial_inference(x)))\n",
    "recurrent_inference = hk.without_apply_rng(hk.transform(lambda x, y: Model().recurrent_inference(x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = hk.data_structures.merge(\n",
    "    initial_inference.init(key, jnp.ones(dim_image)),\n",
    "    recurrent_inference.init(key, jnp.ones(dim_repr), jnp.ones(dim_actions))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state = game.new_initial_state()\n",
    "image = np.array(game_state.observation_tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_repr = initial_inference.apply(params, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ast' from '/usr/lib/python3.8/ast.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
